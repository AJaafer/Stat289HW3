\documentclass[12pt]{article} 
\usepackage{amsmath,graphicx,float}

%\leftmargin{0cm}
\setlength{
\textheight}{8.5in}

% \def\home{/home/stroud}
% \def\sty{\home/latex}
% \def\bib{\home/bbl}
% \def\picdir{/home/stroud/nonlin/pics}
\setlength{
\textheight}{8.5in} \setlength{
\topmargin}{-0.5in} \setlength{
\oddsidemargin}{-.25truein} \setlength{
\evensidemargin}{0truein} \setlength{\parindent}{0in} \setlength{\parskip}{.3cm} 
\renewcommand{\baselinestretch}{1.10} \setlength{
\textwidth}{6.93truein} \leftskip 0truein

%%%%%% boldface characters %%%%%% 
\def\y{\mathbf{y}} 
\newcommand{\be}{\mbox{\boldmath $\beta$}} 
\newcommand{\Si}{\mbox{\boldmath $\Sigma$}} 
\newcommand{\si}{\mbox{\boldmath $\sigma$}} 
\newcommand{\bth}{\mbox{\boldmath $\theta$}}
\newcommand{\cl}{\mathcal{l}}

% TODO define \g shortcut for \vert
\begin{document}
\begin{center}
	{\large {\underline {\bf Eric Groo, Stat 289 Homework 3}}} 
\end{center}

\vspace{.2cm}

\vspace{.2cm} 
\noindent For Questions 1-7 we're considering an observed effect $y_i$, measured for each program $i$, as well as a standard error $\sigma_i$ of the treatment effect. We want to analyze these treatments under a hierarchical model where 
\begin{eqnarray*}
	y_i & \sim & \mbox{Normal}(\theta_i,\sigma^2_i)\\
	\theta_i & \sim & \mbox{Normal}(\mu,\tau^2) 
\end{eqnarray*}
We will assume that $\si^2=(\sigma^2_1,\ldots,\sigma^2_n)$ are all known without error. We are interested in posterior inference for each $\theta_i$, which is the underlying true treatment effect for program $i$, as well as the common treatment mean $\mu$ and variance $\tau^2$.
\begin{enumerate}
	{\leftmargin=1em}
	
	% TODO swap \pi for p to denote priors
	\item Verify that using a flat prior of $p(\mu,\tau)\propto 1$ corresponds to a prior for $(\mu,\tau^2)$ of $p(\mu,\tau^2) \propto \tau^{-1}$. With this prior, write out the full posterior distribution of the unknown parameters $\mu$, $\tau^2$ and $\bth=(\theta_1,\ldots,\theta_n)$. 
	\begin{enumerate}
		\item A simple transformation of $u=\tau^2$ verifies that $p(\mu,\tau)\propto 1$ corresponds to a prior for $(\mu,\tau^2)$ of $p(\mu,\tau^2) \propto \tau^{-1}$. 
		\begin{align*}
			u = \tau^2 &\Rightarrow \tau=\sqrt{u} &  \mathrm{note: } \ 0<\tau<\infty \\
			& \Rightarrow \dfrac{d\tau}{d u} = |\dfrac{1}{2 \sqrt{u}}| \\
			& \Rightarrow p_u (u) = p_\tau (\tau(u)) * |\dfrac{1}{2 \sqrt{u}}| \ 
		\end{align*}
		This implies for $(\mu,\tau^2)$ 
		\begin{align*}
			p_u (\tau^2) & = 1 * |\dfrac{1}{2 \sqrt{\tau^2}}| \\
			& = |\dfrac{1}{2 \tau } | \\
			& \propto \frac{1}{\tau} 
		\end{align*}
		
		\item Assuming that the specified prior is $p(\mu,\tau^2) \propto \tau^{-1}$, the corresponding posterior is
		\[
			\pi(\bth, \mu, \tau^2 | \mathbf{y}, \si^2) \propto \prod_{i=1}^8  \frac{1}{\sqrt{2\pi} \sigma_{i}} \exp \left( \frac{(y_i - \theta_i)^2}{2\sigma_{i}^2} \right) 
			* \prod_{i=1}^8 \frac{1}{\sqrt{2\pi} \tau} \exp\left(\frac{(\theta_i - \mu)^2}{2\tau^2} \right) * \frac{1}{\tau} 
		\]
	\end{enumerate}


% TODO replace \mboxes for distribution with macros and cleaner shortcuts
	\item The first approach is based on the realization that we can actually integrate out each $\theta_i$ from the above distribution, giving us the marginal distribution of our treatment effects $y_i$: $$ y_i \sim \mbox{Normal}(\mu,\sigma^2_i+\tau^2) $$ 
	
	The marginal posterior distribution of $p(\mu,\tau^2|\y,\si^2)$ can be factored into a proportional product of the prior and the marginal distribution of the treatment effects:
		\begin{align}
			p(\mu,\tau^2|\y, \si^2) & \propto p(\mu,\tau^2) p(\y | \mu,\tau^2) \nonumber\\
			& = \frac{1}{\tau} \prod_{i=1}^8 \mbox{Normal}(y_i | \mu, \sigma_{i}^2 + \tau^2)
		\end{align}
	
	\begin{figure}[h]
		% \caption{needs a caption}
		\centering
			\includegraphics[width=0.6\textwidth]{img/2jointlikelihood.pdf}
	\end{figure}
	
	
	\item Use the grid sampling method to get 1000 samples from $p(\mu,\tau^2|\y,\si^2)$. Calculate the mean, median and 95\% posterior interval for $\mu$ and $\tau^2$.
	\begin{center}
		\begin{tabular}{c|ccc}
		         & Mean & Median &  95\% Interval  \\
		\hline
		$\mu$    & 7.75 & 7.72 & $(-0.07, 16.42)$\\
		$\tau^2$ & 3.34 & 2.57 & $(0.03, 9.37)$\\
		\end{tabular}	
	\end{center}
	
	\item The parameters $\theta_i$ are independent in the prior distribution, given $\mu$ and $\tau$. As a result $p(\mathbf{\theta}|\mu,\tau^2,\y,\si^2)$ factors into $i$ components. Conditional on the hyperparameters, these are independent, normally distributed means with normal prior distributions.  We derived the results for this model in class: \[
		\theta \vert \mu, \tau^2, \mathbf{y}, \mathbf{\sigma^2} \sim \mbox{Normal}(\hat{\theta_i}, V_i)
	\]
	where\[
		\hat{\theta_i} = \dfrac{\frac{1}{\sigma^2_i}y_i + \frac{1}{\tau^2}\mu}{V_i} \qquad \mathrm{ and } \qquad V_i=\dfrac{1}{\frac{1}{\sigma^2_i} + \frac{1}{\tau^2}}
	\]
	
	 Using the samples of $\mu$ and $\tau^2$ to draw 1000 samples of each $\theta_i$, the mean of each $\theta_i$ and paired treatment effect $y_i$ is:
\begin{center}
	\begin{tabular}{c|ccccccccc}
		i & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
		\hline
		$\theta_i$ & 8.06 & 7.82 & 7.67 & 7.71 &  7.41 & 7.64 &  8.029 & 7.82 \\
		$y_i$ &  28.0 &   8.0 &  -3.0 &   7.0 &  -1.0 &   1.0 &  18.0 &  12.0 \\
	\end{tabular}
\end{center}
It's clear that the posterior predictive means are more stable, and on average slightly lower than the paired treatment effects $y$.

	
	\item For Questions 5-7, consider $\tau^2$ to be fixed and equal to $\tau^2 = \mbox{median}(\tau^2)$, calculated in question 3. For the sake on convenience I will denote this $\tau_0^2$. We now use a Gibbs sampler to draw samples from the posterior distribution $p(\mu,\bth|\y,\si^2)$. The conditional distributions of the remaining unknown parameters given the other parameters are as follows:
	\begin{align}
		\pi(\bth, \mu | \mathbf{y}, \mathbf{\si^2}, \tau_0^2) & \propto p(\mu) p(\bth|\mu, \tau_0^2) p(\mathbf{y}| \bth, \mathbf{\si^2}) \nonumber \\
		& = \prod_{i=1}^8 N(\theta_i|\mu, \tau_0^2) N(y_i|\theta_i, \sigma_i^2) \\
		\pi(\theta_i|\mu, \mathbf{y}, \mathbf{\si^2}, \tau_0^2) & \propto N(\theta_i|\mu, \tau_0^2) N(y_i|\theta_i, \sigma_i^2) \nonumber \\
		& = N(\theta_i | \hat{\sigma}_i^2, V_i)
	\end{align}
	as before, where \[\hat{\theta_i} = \dfrac{\frac{1}{\sigma^2_i}y_i + \frac{1}{\tau_0^2}\mu}{V_i} \qquad  \mathrm{ and } \qquad V_i=\dfrac{1}{\frac{1}{\sigma^2_i} + \frac{1}{\tau_0^2}}
	\]
	
	\begin{align}
		\pi(\mu | \bth, \mathbf{y}, \mathbf{\si^2}, \tau_0^2) & \propto \prod_{i=1}^8 N( \theta_i \vert \mu, \tau_0^2) \nonumber\\
		& =  N\left(\mu\ \left\vert\right. \frac{\sum\theta_i}{n} ,  \frac{\tau_0^2}{n}\right)
	\end{align}
	
	\item Using a Gibbs sampler based on the conditional distributions from the previous question, I obtained 1000 samples from $p(\mu,\bth|\y,\si^2)$. The code for the problem checks and removes samples before convergence and with high autocorrelation. The following table contains the mean and posterior interval for each $\theta_i$ for the resulting sample:
	
	\begin{center}
		\begin{tabular}{c|ccccccccc}
	 & $\theta_1$ & $\theta_2$ & $\theta_3$ & $\theta_4$ & $\theta_5$ & $\theta_6$ & $\theta_7$ & $\theta_8$ & $\mu$\\
		\hline
	Mean & 11.25 & 8.24 & 6.76 & 8.07 & 6.19 & 6.94 & 10.32 & 8.72 & 8.31\\
	Median & 11.27 & 8.28 & 6.74 & 8.09 & 6.15 & 6.95 & 10.32 & 8.66 & 8.29\\
	Post. Int. & 7.90 & 4.92 & 3.31 & 4.78 & 3.35 & 3.73 & 6.96 & 5.45 & 6.17\\
	 & 14.65 & 11.46 & 10.30 & 11.35 & 9.50 & 10.20 & 13.54 & 12.44 & 10.59\\
		\end{tabular}
	\end{center}

	
	\item I hope I'm not simplifying this too much, but given the above samples of $\bth$ \[
		p(\theta_A > \theta_{\tilde{A}}) = \dfrac{\sum_{i}^n \mathrm{I} \{\theta_{i,A} > \theta_{i,\tilde{A}}\}}{n} \approx 3.1\%
	\] 
	
	\vspace{.05cm} Questions 8-11 are based on the bicycle data. We will focus only on the first two rows of the table (the residential streets with bike routes). We want to model the total amount of traffic $y_i$ on each street (eg. $y_1=74$) as follows:
	\begin{eqnarray*}
		y_i & \sim & \mbox{Poisson}(\theta_i)\\
		\theta_i & \sim & \mbox{Gamma}(\alpha,\beta) 
	\end{eqnarray*}
	
	\item Selecting a flat prior distribution for $\alpha$ and $\beta$, $p(\alpha,\beta) \propto 1$ results in the following posterior distribution of our unknown variables: 
		\begin{align*}
			p(\alpha,\beta,\bth|\y) & \propto p(\alpha,\beta)p(\bth|\alpha,\beta) p(\y|\bth) \\
			& \propto \prod \mbox{Gamma} (\theta_i|\alpha,\beta) \mbox{Poisson}(y_i|\theta_i)
		\end{align*}
	
	\item Given all the other parameters, the conditional distribution of $\theta_i$ is:
	\begin{align*}
		p(\theta_i|\alpha, \beta ,\y) & \propto \mbox{Gamma} (\theta_i|\alpha,\beta) \mbox{Poisson}(y_i|\theta_i) \\
		& \propto \theta_i^{\alpha-1} \exp\{-\beta\theta_i\} \theta_i^{y_i} \exp \{ -\theta_i \} \\
		& = \mbox{Gamma} (\alpha + y_i,\beta + 1)
	\end{align*}
	The justification for the conditional independence of the $\theta_i$'s is essentially the same as in problem 4.
	
	\item The conditional distributions $p \alpha,\beta|\y$ and $p \beta|\alpha,\y$ are not easy to sample from. So, to use a Metropolis step to obtain samples from $p(alpha,\beta|\bth,\y$ I'm going to select the following proposal distributions for $\alpha$ and $\beta$:
	\begin{align*}
		\alpha^* & \sim \mbox{Normal}(\alpha^{(l)}, 1/2) \\
		\beta^* & \sim \mbox{Normal}(\beta^{(l)}, 1/2)
	\end{align*}
	where $l$ is the current step, so $\alpha^{(l)}$ is the draw from the previous step.
	
	\item Combine the algorithm from Question 10 with the conditional distribution from Question 9 to form a Gibbs sampler, which iterates between sampling:
	
	1. sampling $\theta_i | \alpha,\beta,\y$ for each $i$.\\
	2. sampling $\alpha,\beta|\bth,\y$.
	
	Use this algorithm to obtain 1000 samples from $p(\alpha,\beta,\bth|\y)$. Make sure that you only use samples after convergence and that your 1000 samples have low autocorrelation. Calculate means and 95\% posterior intervals for $\alpha$ and $\beta$.
	
	\item Give a 95\% predictive interval for the total traffic on a new residential street with a bike lane. 
\end{enumerate}

\end{document}
